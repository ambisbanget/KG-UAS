{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwfqdzYJtUwE"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet tqdm pandas requests python-dateutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8gY2YAFtYle"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOoqpx1tp9-6"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import time\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "from urllib.parse import urlparse\n",
        "from datetime import datetime\n",
        "from difflib import SequenceMatcher\n",
        "import os\n",
        "\n",
        "TAVILY_API_KEY = \"\"\n",
        "DEEPSEEK_API_KEY = \"\"\n",
        "TAVILY_ENDPOINT = \"https://api.tavily.com/search\"\n",
        "DEEPSEEK_ENDPOINT = \"https://api.deepseek.com/chat/completions\"\n",
        "\n",
        "USE_LLM = bool(DEEPSEEK_API_KEY)\n",
        "\n",
        "EXCEL_PATH = \"/content/drive/MyDrive/KG/UAS/Data_Anggota.xlsx\"\n",
        "SAVE_FOLDER = \"/content/drive/MyDrive/KG/UAS\"\n",
        "os.makedirs(SAVE_FOLDER, exist_ok=True)\n",
        "\n",
        "# =================== DATA ANGGOTA ===================\n",
        "df_anggota = pd.read_excel(EXCEL_PATH)\n",
        "df_anggota = df_anggota.dropna(subset=[\"Nama\"])\n",
        "anggota_dpr = [{\"nama\": str(nama).strip()} for nama in df_anggota[\"Nama\"]]\n",
        "\n",
        "KNOWN_NAMES = [p[\"nama\"] for p in anggota_dpr]\n",
        "KNOWN_NAMES_LC = [n.lower() for n in KNOWN_NAMES]\n",
        "\n",
        "TRUSTED_NEWS = [\n",
        "    \"kompas.com\",\"detik.com\",\"tempo.co\",\"antaranews.com\",\"cnnindonesia.com\",\n",
        "    \"tribunnews.com\",\"liputan6.com\",\"hukumonline.com\",\"jpnn.com\",\"tvonenews.com\",\n",
        "    \"metrotvnews.com\",\"suara.com\",\"vivanews.com\",\"merdeka.com\",\"republika.co.id\"\n",
        "]\n",
        "\n",
        "BLOCKED_DOMAINS = [\n",
        "    \"instagram.com\",\"facebook.com\",\"fb.com\",\"tiktok.com\",\"youtube.com\",\"youtu.be\",\n",
        "    \"twitter.com\",\"x.com\",\"blogspot.\",\"wordpress.\", \"medium.com\"\n",
        "]\n",
        "\n",
        "LEGAL_KEYWORDS = [\n",
        "    \"kasus\",\"korupsi\",\"suap\",\"gratifikasi\",\"pencucian uang\",\"tppu\",\"pidana\",\n",
        "    \"tersangka\",\"terdakwa\",\"dakwa\",\"penyidikan\",\"penyelidikan\",\"ditetapkan\",\n",
        "    \"diperiksa\",\"ditahan\",\"kpk\",\"kejaksaan\",\"polri\",\"ott\",\"pengadilan\",\"vonis\"\n",
        "]\n",
        "\n",
        "HOAX_KEYWORDS = [\"hoaks\",\"hoax\",\"cek fakta\",\"fact check\",\"tidak benar\",\"dibantah\",\"klaim palsu\"]\n",
        "\n",
        "PROMPT_EKSTRAK = \"\"\"\n",
        "Anda adalah sistem ekstraksi fakta hukum yang sangat ketat.\n",
        "Bekerjalah HANYA berdasarkan teks yang diberikan. Jangan menambah informasi.\n",
        "\n",
        "TARGET = \"{nama}\"\n",
        "\n",
        "ATURAN:\n",
        "- Jika berita menyatakan TARGET sebagai 'tersangka' -> status = \"diduga\".\n",
        "- Jika berita menyatakan TARGET sebagai 'terdakwa' -> status = \"diduga\".\n",
        "- Jika berita menyatakan TARGET 'saksi' atau 'diperiksa sebagai saksi' -> status = \"saksi\".\n",
        "- Jika berita adalah hoaks -> tidak diambil\".\n",
        "- Jika hanya komentar/opini -> status = \"tidak diambil\".\n",
        "- Jika ada indikasi belum resmi -> status = \"tidak diambil\".\n",
        "\n",
        "OUTPUT JSON EXACT (tanpa pasal):\n",
        "{{\n",
        "  \"status\": \"...\",\n",
        "  \"jenis_kasus\": \"...\",\n",
        "  \"ringkasan\": \"...\",\n",
        "  \"peran\": \"...\",\n",
        "  \"related_members\": [\"...\"],\n",
        "  \"triple\": [\n",
        "    [\"{nama}\", \"status\", \"...\"],\n",
        "    [\"{nama}\", \"terkait_kasus\", \"...\"],\n",
        "    [\"{nama}\", \"peran\", \"...\"]\n",
        "  ]\n",
        "}}\n",
        "\n",
        "TEKS BERITA:\n",
        "{text}\n",
        "\"\"\"\n",
        "\n",
        "# ======================================================\n",
        "def domain_of(url):\n",
        "    try:\n",
        "        return urlparse(url).netloc.lower()\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "def is_blocked_domain(url):\n",
        "    dom = domain_of(url)\n",
        "    return any(b in dom for b in BLOCKED_DOMAINS)\n",
        "\n",
        "def is_trusted_domain(url):\n",
        "    dom = domain_of(url)\n",
        "    return any(t in dom for t in TRUSTED_NEWS)\n",
        "\n",
        "def name_in_text_adaptive(name_full, title, content):\n",
        "    txt = (title + \" \" + content).lower()\n",
        "    name_lc = name_full.lower().strip()\n",
        "    if name_lc in txt:\n",
        "        return True\n",
        "    first = name_lc.split()[0]\n",
        "    if re.search(r\"\\b\"+re.escape(first)+r\"\\b\", txt):\n",
        "        sents = re.split(r'(?<=[\\.\\?\\!])\\s+', txt)\n",
        "        for s in sents:\n",
        "            if (re.search(r\"\\b\"+re.escape(first)+r\"\\b\", s)\n",
        "                and re.search(r\"dpr|anggota dewan|komisi|fraksi\", s)):\n",
        "                same_first = [n for n in KNOWN_NAMES_LC if n.split()[0] == first and n != name_lc]\n",
        "                if not any(o in s for o in same_first):\n",
        "                    return True\n",
        "    return False\n",
        "\n",
        "def detect_hoax(text):\n",
        "    tl = text.lower()\n",
        "    return any(k in tl for k in HOAX_KEYWORDS)\n",
        "\n",
        "def detect_legal_keyword(text):\n",
        "    tl = text.lower()\n",
        "    return any(k in tl for k in LEGAL_KEYWORDS)\n",
        "\n",
        "def extract_status_and_role(text, target_name):\n",
        "    t = text.lower()\n",
        "\n",
        "    if re.search(r\"(diperiksa|diperiksa sebagai saksi|memberikan keterangan sebagai saksi)\\b\", t):\n",
        "        return \"saksi\", \"saksi\"\n",
        "\n",
        "    if re.search(r\"\\bdiduga\\b|\\bdugaan\\b|\\ditetapkan sebagai tersangka|menjadi tersangka|sebagai tersangka\\|b\\bdiperiksa\\b(sebagai terdakwa|didakwa|terdakwa)\\b\", t):\n",
        "        return \"diduga\", \"pelaku/diduga\"\n",
        "\n",
        "    return None, None\n",
        "\n",
        "def detect_related_members(text, target_name):\n",
        "    tl = text.lower()\n",
        "    related = []\n",
        "    for name_full in KNOWN_NAMES_LC:\n",
        "        if name_full == target_name.lower():\n",
        "            continue\n",
        "        if name_full in tl:\n",
        "            related.append(name_full.title())\n",
        "    return sorted(list(dict.fromkeys(related)))\n",
        "\n",
        "def call_deepseek(prompt):\n",
        "    if not USE_LLM:\n",
        "        return None\n",
        "    try:\n",
        "        payload = {\n",
        "            \"model\": \"deepseek-chat\",\n",
        "            \"messages\": [{\"role\":\"user\",\"content\":prompt}],\n",
        "            \"temperature\": 0.0\n",
        "        }\n",
        "        r = requests.post(DEEPSEEK_ENDPOINT, json=payload,\n",
        "                          headers={\"Authorization\": f\"Bearer {DEEPSEEK_API_KEY}\"}, timeout=20)\n",
        "        r.raise_for_status()\n",
        "        return r.json()\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def analyze_article(title, url, content, target_name):\n",
        "    if is_blocked_domain(url) or not is_trusted_domain(url):\n",
        "        return None\n",
        "    if not name_in_text_adaptive(target_name, title, content):\n",
        "        return None\n",
        "    else:\n",
        "        if not detect_legal_keyword(content):\n",
        "            return None\n",
        "        status, role = extract_status_and_role(content, target_name)\n",
        "        jenis = \"\"\n",
        "        ringkasan = content[:250]\n",
        "\n",
        "    jenis_found = []\n",
        "    for kw in [\"korupsi\",\"suap\",\"gratifikasi\",\"narkoba\",\n",
        "               \"penggelapan\",\"penipuan\",\"pemerasan\",\n",
        "               \"pencucian uang\"]:\n",
        "        if kw in content.lower():\n",
        "            jenis_found.append(kw)\n",
        "    jenis_kasus = \", \".join(jenis_found) if jenis_found else jenis\n",
        "\n",
        "    role_map = {\n",
        "        \"pelaku/diduga\":\"pelaku/diduga\",\n",
        "        \"saksi\":\"saksi\",\n",
        "    }\n",
        "    peran_norm = role_map.get(role, role)\n",
        "\n",
        "    triple = [\n",
        "        [target_name, \"status\", status],\n",
        "        [target_name, \"terkait_kasus\", jenis_kasus],\n",
        "        [target_name, \"peran\", peran_norm]\n",
        "    ]\n",
        "\n",
        "    return {\n",
        "        \"title\": title,\n",
        "        \"url\": url,\n",
        "        \"status\": status,\n",
        "        \"jenis_kasus\": jenis_kasus,\n",
        "        \"ringkasan\": ringkasan,\n",
        "        \"peran\": peran_norm,\n",
        "        \"related_members\": detect_related_members(content, target_name),\n",
        "        \"triple\": triple\n",
        "    }\n",
        "\n",
        "def tavily_search(query, max_results=8):\n",
        "    if not TAVILY_API_KEY:\n",
        "        return []\n",
        "    try:\n",
        "        payload = {\"api_key\": TAVILY_API_KEY, \"query\": query, \"max_results\": max_results}\n",
        "        r = requests.post(TAVILY_ENDPOINT, json=payload, timeout=15)\n",
        "        r.raise_for_status()\n",
        "        return r.json().get(\"results\", [])\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "def similar(a, b):\n",
        "    return SequenceMatcher(None, a.lower(), b.lower()).ratio()\n",
        "\n",
        "def dedupe_articles(articles):\n",
        "    if not articles:\n",
        "        return []\n",
        "    result = []\n",
        "    status_priority = {\"tersangka\":5,\"terdakwa\":4,\"saksi\":3,\"diperiksa\":3,\"diduga\":2,\"bukan terlibat\":1,\"tidak terlibat (hoaks)\":0}\n",
        "    for a in articles:\n",
        "        keep = True\n",
        "        for b in result:\n",
        "            if similar(a[0], b[0]) > 0.85:\n",
        "                if status_priority.get(a[3],0) > status_priority.get(b[3],0):\n",
        "                    b[:] = a\n",
        "                keep = False\n",
        "                break\n",
        "        if keep:\n",
        "            result.append(a)\n",
        "    return result\n",
        "\n",
        "def is_valid_name(text, target_name):\n",
        "    pattern = r'\\b{}\\b'.format(re.escape(target_name))\n",
        "    return re.search(pattern, text, flags=re.IGNORECASE) is not None\n",
        "\n",
        "def filter_news(results, target_name):\n",
        "    seen_urls = set()\n",
        "    seen_titles = set()\n",
        "    filtered = []\n",
        "\n",
        "    for news in results:\n",
        "        title = news.get('title', '')\n",
        "        url = news.get('url', '')\n",
        "        domain = urlparse(url).netloc.lower()\n",
        "\n",
        "        if not any(t in domain for t in TRUSTED_NEWS):\n",
        "            continue\n",
        "        if not is_valid_name(title + \" \" + news.get('summary', ''), target_name):\n",
        "            continue\n",
        "        if url in seen_urls or title in seen_titles:\n",
        "            continue\n",
        "\n",
        "        seen_urls.add(url)\n",
        "        seen_titles.add(title)\n",
        "        filtered.append(news)\n",
        "\n",
        "    return filtered\n",
        "\n",
        "def run_all(save_prefix=\"results\"):\n",
        "    save_path = \"/content/drive/MyDrive/KG/UAS\"\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "    outputs = []\n",
        "    for person in anggota_dpr:\n",
        "        name = person[\"nama\"]\n",
        "        print(\"=\"*70)\n",
        "        print(f\"Mengecek kasus hukum untuk: {name}\")\n",
        "\n",
        "        queries = [\n",
        "            f\"{name} kasus\",\n",
        "            f\"{name} diperiksa KPK\",\n",
        "            f\"{name} tersangka\",\n",
        "            f\"{name} dugaan korupsi\"\n",
        "        ]\n",
        "\n",
        "        raw_results = []\n",
        "        for q in queries:\n",
        "            hits = tavily_search(q, max_results=6)\n",
        "            for h in hits:\n",
        "                raw_results.append({\n",
        "                    \"title\": h.get(\"title\",\"\"),\n",
        "                    \"url\": h.get(\"url\",\"\"),\n",
        "                    \"summary\": h.get(\"content\",\"\")\n",
        "                })\n",
        "            time.sleep(0.2)\n",
        "\n",
        "        valid_results = filter_news(raw_results, name)\n",
        "\n",
        "        deduped = []\n",
        "        for item in valid_results:\n",
        "            t = item[\"title\"]\n",
        "            u = item[\"url\"]\n",
        "            c = item[\"summary\"]\n",
        "            deduped.append([t, u, c, \"\"])\n",
        "\n",
        "        deduped = dedupe_articles(deduped)\n",
        "\n",
        "        valid_count = 0\n",
        "        for t,u,c,_ in deduped:\n",
        "            art = analyze_article(t, u, c, name)\n",
        "            if not art:\n",
        "                continue\n",
        "            valid_count += 1\n",
        "            outputs.append({\n",
        "                \"target\": name,\n",
        "                \"title\": art[\"title\"],\n",
        "                \"url\": art[\"url\"],\n",
        "                \"status\": art[\"status\"],\n",
        "                \"jenis_kasus\": art[\"jenis_kasus\"],\n",
        "                \"ringkasan\": art[\"ringkasan\"],\n",
        "                \"peran\": art[\"peran\"],\n",
        "                \"related_members\": \";\".join(art[\"related_members\"]),\n",
        "                \"triple\": json.dumps(art[\"triple\"], ensure_ascii=False)\n",
        "            })\n",
        "\n",
        "            print()\n",
        "            print(f\"- {art['title']}\")\n",
        "            print(f\"  {art['url']}\")\n",
        "            print(f\"  Status      : {art['status']}\")\n",
        "            print(f\"  Jenis Kasus : {art['jenis_kasus']}\")\n",
        "            print(f\"  Ringkasan   : {art['ringkasan'][:300]}\")\n",
        "            print(f\"  Peran       : {art['peran']}\")\n",
        "            if art[\"related_members\"]:\n",
        "                print(f\"  Related DPR : {', '.join(art['related_members'])}\")\n",
        "            print(\"  Triple:\")\n",
        "            for tr in art[\"triple\"]:\n",
        "                print(\"    \", tr)\n",
        "            print()\n",
        "\n",
        "        print(f\"Ditemukan {valid_count} berita valid untuk {name}.\\n\")\n",
        "\n",
        "    if outputs:\n",
        "        ts = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        json_file = os.path.join(save_path, f\"{save_prefix}_{ts}.json\")\n",
        "        csv_file = os.path.join(save_path, f\"{save_prefix}_{ts}.csv\")\n",
        "        with open(json_file,\"w\",encoding=\"utf-8\") as f:\n",
        "            json.dump(outputs, f, ensure_ascii=False, indent=2)\n",
        "        pd.DataFrame(outputs).to_csv(csv_file, index=False, encoding=\"utf-8-sig\")\n",
        "        print(f\"Saved {len(outputs)} rows â†’ {json_file}, {csv_file}\")\n",
        "    else:\n",
        "        print(\"No validated outputs to save.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_all()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
